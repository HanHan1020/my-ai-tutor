import streamlit as st
import os
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings, 
    StorageContext, 
    load_index_from_storage
)
from llama_index.core.memory import ChatMemoryBuffer

# --- 1. ç¶²é ä»‹é¢èˆ‡ API é‡‘é‘°é…ç½® ---
st.set_page_config(page_title="ç³»çµ±å‹•åŠ›å­¸æ™ºæ…§åŠ©æ•™", layout="wide", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ğŸ¤–ç³»çµ±å‹•åŠ›å­¸ï¼šæ™ºæ…§æ•™å­¸ç³»çµ±")

# å¾ Streamlit Secrets è®€å– API Key (éƒ¨ç½²å¾Œè¨­å®š)
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âŒ æ‰¾ä¸åˆ° GOOGLE_API_KEYï¼è«‹åœ¨ Secrets ä¸­è¨­å®šã€‚")
    st.stop()

# è·¯å¾‘å®šç¾©
PERSIST_DIR = "./storage"
DATA_DIR = "./data"

# --- 2. æ ¸å¿ƒå¼•æ“åˆå§‹åŒ– (ä½¿ç”¨ Gemini) ---
@st.cache_resource
def init_expert_system():
    # 1. çµ±ä¸€ä½¿ç”¨ model_name åƒæ•¸
    # 2. ç§»é™¤å­—ä¸²å‰çš„ models/ å‰ç¶´ (é¿å…è·¯å¾‘é‡è¤‡ç–ŠåŠ å°è‡´ 404)
    Settings.llm = Gemini(model_name="gemini-1.5-flash", api_key=GOOGLE_API_KEY)
    Settings.embed_model = GeminiEmbedding(model_name="text-embedding-004", api_key=GOOGLE_API_KEY)
    
    # æŒä¹…åŒ–é‚è¼¯
    if not os.path.exists(PERSIST_DIR):
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        index = VectorStoreIndex.from_documents(documents)
        index.storage_context.persist(persist_dir=PERSIST_DIR)
    else:
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        index = load_index_from_storage(storage_context)
    
    course_outline = "1.å»ºæ¨¡æµç¨‹ 2.å› æœç’°è·¯åœ– 3.å­˜é‡æµé‡åœ– 4.å»¶é²èˆ‡Little's Law 5.æ¨¡å‹é©—è­‰"

    memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        memory=memory,
        system_prompt=(
            f"ä½ æ˜¯ä¸€ä½å°ˆå®¶æ•™æˆã€‚1.åš´ç¦èªªã€æ ¹æ“šæ•™æã€ã€‚2.å…§åŒ–çŸ¥è­˜ç›´æ¥å›ç­”ã€‚3.åƒ…é™ç³»çµ±å‹•åŠ›å­¸ç¯„åœã€‚\nå¤§ç¶±ï¼š{course_outline}\n4.é™ç¹é«”ä¸­æ–‡ã€‚"
        )
    )
    return chat_engine

chat_engine = init_expert_system()

# --- 3. å°è©±ä»‹é¢ (å…¶é¤˜é‚è¼¯èˆ‡ä¹‹å‰ç›¸åŒ) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = chat_engine.chat(prompt + " (è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”)")
        st.markdown(str(response))
        st.session_state.messages.append({"role": "assistant", "content": str(response)})