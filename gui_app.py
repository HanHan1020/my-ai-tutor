import streamlit as st
import os
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from llama_index.core import (
    VectorStoreIndex, 
    SimpleDirectoryReader, 
    Settings, 
    StorageContext, 
    load_index_from_storage
)
from llama_index.core.memory import ChatMemoryBuffer

# --- 1. ç¶²é  UI èˆ‡ API é…ç½® ---
st.set_page_config(page_title="ç³»çµ±å‹•åŠ›å­¸æ•™æˆ", layout="wide", page_icon="ğŸ‘¨â€ğŸ«")
st.title("ğŸ¤– ç³»çµ±å‹•åŠ›å­¸æ™ºæ…§å°å¸«ç³»çµ±")

# å¾ Streamlit Secrets è®€å– API Key
try:
    # ç¢ºä¿ Secrets ä¸­çš„åç¨±èˆ‡æ­¤è™•ä¸€è‡´
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except Exception:
    st.error("âŒ æ‰¾ä¸åˆ° GOOGLE_API_KEYï¼è«‹åœ¨ Streamlit Cloud çš„ Advanced settings > Secrets ä¸­è¨­å®šã€‚")
    st.stop()

# å®šç¾©è³‡æ–™è·¯å¾‘
PERSIST_DIR = "./storage"
DATA_DIR = "./data"

# --- 2. æ ¸å¿ƒå¼•æ“åˆå§‹åŒ– (å°ˆå®¶è§’è‰²èˆ‡ Gemini API) ---
@st.cache_resource
def init_expert_system():
    # A. æ¨¡å‹é…ç½® (ä¿®æ­£å¾Œçš„æœ€æ–°ç›¸å®¹æ ¼å¼)
    # ä½¿ç”¨ model_name é¿å… 404 éŒ¯èª¤ï¼Œä¸¦åŠ å…¥ latest ç¢ºä¿è³‡æºå­˜å–
    Settings.llm = Gemini(
        model_name="models/gemini-1.5-flash-latest", 
        api_key=GOOGLE_API_KEY
    )
    Settings.embed_model = GeminiEmbedding(
        model_name="models/text-embedding-004", 
        api_key=GOOGLE_API_KEY
    )
    
    # B. ç´¢å¼•æŒä¹…åŒ–é‚è¼¯
    if not os.path.exists(PERSIST_DIR):
        if not os.path.exists(DATA_DIR) or not os.listdir(DATA_DIR):
            st.error(f"âŒ æ‰¾ä¸åˆ°æ•™æï¼è«‹ç¢ºä¿ GitHub çš„ '{DATA_DIR}' è³‡æ–™å¤¾å…§æœ‰ PDFã€‚")
            st.stop()
        with st.spinner("æ•™æˆæ­£åœ¨é–±è®€æ•™æä¸¦å»ºç«‹çŸ¥è­˜é«”ç³»..."):
            documents = SimpleDirectoryReader(DATA_DIR).load_data()
            index = VectorStoreIndex.from_documents(documents)
            index.storage_context.persist(persist_dir=PERSIST_DIR)
    else:
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        index = load_index_from_storage(storage_context)
    
    # C. é…ç½®å°è©±å¼•æ“ (å°ˆå®¶ Persona + åš´æ ¼èªè¨€ç´„æŸ)
    memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        memory=memory,
        system_prompt=(
            "ä½ æ˜¯ä¸€ä½å…·å‚™ 30 å¹´æ•™å­¸ç¶“é©—çš„ã€ç³»çµ±å‹•åŠ›å­¸æ¬Šå¨æ•™æˆã€ã€‚è«‹éµå®ˆä»¥ä¸‹æœ€é«˜æŒ‡å°åŸå‰‡ï¼š\n"
            "1. èªè¨€é–å®šï¼šä½ åªä½¿ç”¨ã€å°ç£ç¹é«”ä¸­æ–‡ã€å›ç­”ã€‚åš´ç¦ä½¿ç”¨ç°¡é«”å­—ã€å¤§é™¸ç”¨èªï¼ˆå¦‚è³ªé‡ã€å„ªåŒ–ã€æ‰“å°ï¼‰ã€‚\n"
            "2. æ¶ˆé™¤å†—é¤˜ï¼šç›´æ¥å›ç­”å•é¡Œï¼Œåš´ç¦ä½¿ç”¨ã€æ ¹æ“šæä¾›çš„æ•™æã€ã€ã€æ ¹æ“šä¸Šä¸‹æ–‡ã€ç­‰ç”Ÿç¡¬é–‹å ´ç™½ã€‚å°‡çŸ¥è­˜è¦–ç‚ºä½ è…¦ä¸­çš„å…§åœ¨æ™ºæ…§ã€‚\n"
            "3. å°ˆæ¥­å®ˆå‰‡ï¼šå›ç­”åƒ…é™æ–¼ç³»çµ±å‹•åŠ›å­¸ã€‚è‹¥å•é¡Œç„¡é—œï¼Œè«‹åˆ—å‡ºæ•™å­¸å¤§ç¶±ï¼ˆCh 3, 5, 6, 11ï¼‰ä¸¦å¼•å°å›èª²ç¨‹ã€‚\n"
            "4. çµæ§‹åŒ–å›ç­”ï¼šå…·å‚™å­¸è¡“æ·±åº¦ï¼Œé‡è¦å°ˆæœ‰åè©åŠ è¨»è‹±æ–‡ã€‚"
        )
    )
    return chat_engine

# å•Ÿå‹•ç³»çµ±
chat_engine = init_expert_system()

# --- 3. å°è©±ä»‹é¢èˆ‡æ­·å²ç´€éŒ„ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# é¡¯ç¤ºæ­·å²å°è©±
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "sources" in msg:
            with st.expander("ğŸ” æ•™æˆå¼•ç”¨çš„æ–‡ç»å‡ºè™•"):
                st.write(msg["sources"])

# ä½¿ç”¨è€…è¼¸å…¥
if prompt := st.chat_input("å‘æ•™æˆè«‹æ•™é—œæ–¼æ•™æçš„å…§å®¹..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("æ•™æˆæ­£åœ¨æ§‹æ€å°ˆæ¥­çš„å›è¦†..."):
            # åœ¨ prompt å¾Œç«¯è‡ªå‹•è¿½åŠ éš±å½¢æŒ‡ä»¤ï¼Œå¼·åŒ–èªè¨€é–å®š
            response = chat_engine.chat(prompt + " (æ³¨æ„ï¼šè«‹å‹™å¿…ä»¥ç¹é«”ä¸­æ–‡å›ç­”ï¼Œåš´ç¦ç°¡é«”)")
            answer = str(response)
            
            # æå–ä¾†æºè³‡è¨Š (ç”¨æ–¼å­¸è¡“é©—è­‰)
            ref_info = ""
            if hasattr(response, 'source_nodes'):
                for i, node in enumerate(response.source_nodes):
                    fname = node.metadata.get('file_name', 'æœªçŸ¥ç« ç¯€')
                    score = f"{node.score:.2f}" if node.score else "N/A"
                    ref_info += f"**[æ–‡ç»ç‰‡æ®µ {i+1}]** `{fname}` (é—œè¯æ¬Šé‡: {score})\n\n"
            
            st.markdown(answer)
            if ref_info:
                with st.expander("ğŸ” æ•™æˆå¼•ç”¨çš„æ–‡ç»å‡ºè™•"):
                    st.write(ref_info)
            
            st.session_state.messages.append({
                "role": "assistant", 
                "content": answer, 
                "sources": ref_info
            })